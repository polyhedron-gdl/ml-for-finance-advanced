{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8b139d",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbadd0b",
   "metadata": {},
   "source": [
    "## Example 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fe5d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbc64e",
   "metadata": {},
   "source": [
    "[ref](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/blob/master/chapter04/grid_world.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce2858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76cbaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'Agg' backend for matplotlib, which is great for scripts generating files without displaying them.\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Define the size of the gridworld.\n",
    "WORLD_SIZE = 4\n",
    "# Define possible actions in the gridworld: left, up, right, down.\n",
    "ACTIONS = [np.array([0, -1]),  # left\n",
    "           np.array([-1, 0]),  # up\n",
    "           np.array([0, 1]),   # right\n",
    "           np.array([1, 0])]   # down\n",
    "# Probability of each action being taken. The agent follows the equiprobable random policy.\n",
    "ACTION_PROB = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a7d8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(state):\n",
    "    \"\"\"Check if the current state is a terminal state.\"\"\"\n",
    "    x, y = state\n",
    "    # Terminal states are top-left and bottom-right corners of the grid.\n",
    "    return (x == 0 and y == 0) or (x == WORLD_SIZE - 1 and y == WORLD_SIZE - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1cc3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    \"\"\"Calculate the next state and reward given the current state and action.\"\"\"\n",
    "    if is_terminal(state):\n",
    "        return state, 0  # No reward and remain in the same state if terminal.\n",
    "\n",
    "    # Compute the next state based on current state and action.\n",
    "    next_state = (np.array(state) + action).tolist()\n",
    "    x, y = next_state\n",
    "\n",
    "    # If next state is outside the grid, revert to the original state.\n",
    "    if x < 0 or x >= WORLD_SIZE or y < 0 or y >= WORLD_SIZE:\n",
    "        next_state = state\n",
    "\n",
    "    reward = -1  # Reward for each transition.\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "021e7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(image):\n",
    "    \"\"\"Create a plot to visualize the values of each state in the grid.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax, bbox=[0, 0, 1, 1])\n",
    "\n",
    "    nrows, ncols = image.shape\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    # Populate the table with the state values.\n",
    "    for (i, j), val in np.ndenumerate(image):\n",
    "        tb.add_cell(i, j, width, height, text=val, loc='center', facecolor='white')\n",
    "\n",
    "    # Add row and column labels.\n",
    "    for i in range(len(image)):\n",
    "        tb.add_cell(i, -1, width, height, text=i+1, loc='right', edgecolor='none', facecolor='none')\n",
    "        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center', edgecolor='none', facecolor='none')\n",
    "    ax.add_table(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "551dc14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_value(in_place=True, discount=1.0):\n",
    "    \"\"\"Compute the state values using iterative policy evaluation.\"\"\"\n",
    "    new_state_values = np.zeros((WORLD_SIZE, WORLD_SIZE))  # Initialize state values to zero.\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        if in_place:\n",
    "            state_values = new_state_values  # In-place updates.\n",
    "        else:\n",
    "            state_values = new_state_values.copy()  # Out-of-place updates.\n",
    "\n",
    "        old_state_values = state_values.copy()\n",
    "\n",
    "        for i in range(WORLD_SIZE):\n",
    "            for j in range(WORLD_SIZE):\n",
    "                value = 0\n",
    "                for action in ACTIONS:\n",
    "                    (next_i, next_j), reward = step([i, j], action)\n",
    "                    value += ACTION_PROB * (reward + discount * state_values[next_i, next_j])\n",
    "                new_state_values[i, j] = value\n",
    "\n",
    "        max_delta_value = abs(old_state_values - new_state_values).max()\n",
    "        if max_delta_value < 1e-4:\n",
    "            break\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return new_state_values, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13e65a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_4_1():\n",
    "    \"\"\"Reproduce Figure 4.1 from Sutton and Barto's book.\"\"\"\n",
    "    _, async_iteration = compute_state_value(in_place=True)\n",
    "    values, sync_iteration = compute_state_value(in_place=False)\n",
    "    draw_image(np.round(values, decimals=2))\n",
    "    print('In-place: {} iterations'.format(async_iteration))\n",
    "    print('Synchronous: {} iterations'.format(sync_iteration))\n",
    "\n",
    "    plt.savefig('./images/figure_4_1.png')  # Save the figure to a file.\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91fb3795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-place: 113 iterations\n",
      "Synchronous: 172 iterations\n"
     ]
    }
   ],
   "source": [
    "figure_4_1()  # Run the function when the script is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2589bb24",
   "metadata": {},
   "source": [
    "### The Bellman Equation for Policy Evaluation\n",
    "\n",
    "In the provided Python code, the Bellman equation is applied within the `compute_state_value` function, specifically in the nested loop where the value of each state in the gridworld is calculated based on potential actions. Let’s break down how this section corresponds to the Bellman equation for policy evaluation.\n",
    "\n",
    "The Bellman equation for policy evaluation in its simplest form states:\n",
    "\n",
    "$$\n",
    "V(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma V(s') \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $V(s)$ is the state-value function for state $s$,\n",
    "- $\\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\\pi$,\n",
    "- $p(s', r | s, a)$ is the probability of transitioning to state $s'$ and receiving reward $r$ after taking action $a$ in state $s$,\n",
    "- $r$ is the reward received after transitioning from $s$ to $s'$,\n",
    "- $\\gamma$ is the discount factor,\n",
    "- $V(s')$ is the value of the next state.\n",
    "\n",
    "### Application in the Code\n",
    "\n",
    "The implementation in the provided code uses an equally likely policy, where each action is selected with equal probability ($\\pi(a|s) = 0.25$ for each action). Here’s how the specific part of the code maps to the Bellman equation:\n",
    "\n",
    "```python\n",
    "for i in range(WORLD_SIZE):\n",
    "    for j in range(WORLD_SIZE):\n",
    "        value = 0\n",
    "        for action in ACTIONS:\n",
    "            (next_i, next_j), reward = step([i, j], action)\n",
    "            value += ACTION_PROB * (reward + discount * state_values[next_i, next_j])\n",
    "        new_state_values[i, j] = value\n",
    "```\n",
    "\n",
    "- **`value = 0`**: This initializes the sum for the current state’s value calculation.\n",
    "- **`for action in ACTIONS`**: This loop over actions represents the sum over actions $a$ in the equation.\n",
    "- **`(next_i, next_j), reward = step([i, j], action)`**: This function call calculates the next state and reward given the current state $[i, j]$ and action. It directly corresponds to $s'$ and $r$ in the equation.\n",
    "- **`ACTION_PROB`**: This is $\\pi(a|s)$, the probability of selecting each action which is 0.25, indicating an equal likelihood of choosing any action.\n",
    "- **`(reward + discount * state_values[next_i, next_j])`**: This is the term $[r + \\gamma V(s')]$ in the Bellman equation, where `reward` is $r$ and `state_values[next_i, next_j]` corresponds to $V(s')$.\n",
    "- **`discount`**: This is the discount factor $\\gamma$ in the equation, which scales the value of future rewards.\n",
    "- **`new_state_values[i, j] = value`**: This assigns the computed value back to the state-value array for state $(i, j)$, effectively updating $V(s)$ for each state $s$ based on the Bellman equation.\n",
    "\n",
    "The Bellman equation is used to iteratively update the value of each state based on the possible actions and the subsequent states and rewards. This iterative process is central to many reinforcement learning algorithms, especially those involving policy evaluation and improvement like in this example. The Bellman equation ensures that the value of each state properly accounts for the expected returns under the given policy, leading to the convergence of state values toward the true values as the algorithm iterates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f9a92",
   "metadata": {},
   "source": [
    "In the context of this simple example, which represents a typical gridworld scenario from reinforcement learning literature, $ p(s', r \\mid s, a) = 1 $ for all states $s$ and actions $a$ because the environment is deterministic. In a deterministic environment, the outcome of each action taken in a state is completely predictable. There are no random elements affecting where you end up or what reward you get after taking an action. Therefore, when an action $a$ is taken in a state $s$, there is only one possible outcome state $s'$ and one possible reward $r$. Since each action from a state leads to a specific next state and specific reward deterministically, the transition probability is 1. There are no other possibilities to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c4916",
   "metadata": {},
   "source": [
    "## A Simple Gridworld Example\n",
    "\n",
    "Gridworld Setup:\n",
    "\n",
    "- A $n \\times n$ grid with states labeled from 0 to $n$.\n",
    "- Actions possible are ***up, down, left***, and ***right***.\n",
    "- Moving out of the grid keeps you in the same state.\n",
    "- A reward of -1 on all transitions except those leading to the terminal state (state $n$), which has a reward of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53cbcf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values for each state in a 4x4 grid:\n",
      "-4.69 -4.10 -3.44 -2.71\n",
      "-4.10 -3.44 -2.71 -1.90\n",
      "-3.44 -2.71 -1.90 -1.00\n",
      "-2.71 -1.90 -1.00 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#\n",
    "# next_state Function: This computes the next state given a state and an action. \n",
    "# It handles edge cases to ensure the state stays within grid boundaries. \n",
    "#\n",
    "def next_state(s, a, grid_size):\n",
    "    \"\"\"Determine the next state given action and current state.\"\"\"\n",
    "    if a == 'up':  # Move up unless at the top row\n",
    "        return s if s < grid_size else s - grid_size\n",
    "    elif a == 'down':  # Move down unless at the bottom row\n",
    "        return s if s >= grid_size * (grid_size - 1) else s + grid_size\n",
    "    elif a == 'left':  # Move left unless at the left edge\n",
    "        return s if s % grid_size == 0 else s - 1\n",
    "    elif a == 'right':  # Move right unless at the right edge\n",
    "        return s if (s + 1) % grid_size == 0 else s + 1\n",
    "#\n",
    "# value_iteration Function: Implements value iteration. It iterates over each state, \n",
    "# updating the value function by considering all possible actions and choosing the one \n",
    "# that maximizes the state's value until the maximum change across all states between \n",
    "# iterations is less than a small threshold.\n",
    "#\n",
    "# Terminal State: In our setup, the lower right corner of the grid is terminal with a \n",
    "# reward of 0 and does not transition anywhere.\n",
    "#\n",
    "def value_iteration(grid_size, gamma=0.9, threshold=0.001):\n",
    "    states = np.arange(grid_size**2)           # Array of all states from 0 to grid_size^2 - 1\n",
    "    V = np.zeros(len(states))                  # Initialize value function array to zero\n",
    "    delta = float('inf')                       # Large initial value for tracking convergence\n",
    "    actions = ['up', 'down', 'left', 'right']  # Possible actions\n",
    "    while delta > threshold:                   # Continue until change in value function is small\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if s == grid_size**2 - 1:  # Skip terminal state (assumed to be the last state)\n",
    "                continue\n",
    "            v = V[s]  # Current value of state s\n",
    "            # Compute the maximum value of all actions from state s\n",
    "            V[s] = max([sum([p * (r + gamma * V[next_state(s, a, grid_size)]) for p, r in [(1.0, -1)]])\n",
    "                        for a in actions])\n",
    "            # Update delta to be the maximum difference observed in this iteration\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "    return V  # Return the final value function\n",
    "\n",
    "# Example usage:\n",
    "grid_size = 4  # Change this to any grid size\n",
    "optimal_values = value_iteration(grid_size)  # Run value iteration\n",
    "\n",
    "print(\"Optimal values for each state in a {}x{} grid:\".format(grid_size, grid_size))\n",
    "for i in range(grid_size):\n",
    "    print(' '.join(\"{:.2f}\".format(val) for val in optimal_values[i*grid_size:(i+1)*grid_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9b82a",
   "metadata": {},
   "source": [
    "## Alternative Implementation (doesn't work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bad8f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values for each state in a 4x4 grid:\n",
      "['0.00', '-1.00', '-1.99', '-2.97']\n",
      "['-1.00', '-1.99', '-2.97', '-1.99']\n",
      "['-1.99', '-2.97', '-1.99', '-1.00']\n",
      "['-2.97', '-1.99', '-1.00', '0.00']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def next_state(s, a, grid_size):\n",
    "    \"\"\"Determine the next state given action and current state.\"\"\"\n",
    "    if a == 'up':\n",
    "        return s if s < grid_size else s - grid_size\n",
    "    elif a == 'down':\n",
    "        return s if s >= grid_size * (grid_size - 1) else s + grid_size\n",
    "    elif a == 'left':\n",
    "        return s if s % grid_size == 0 else s - 1\n",
    "    elif a == 'right':\n",
    "        return s if (s + 1) % grid_size == 0 else s + 1\n",
    "    return s\n",
    "\n",
    "def value_iteration(grid_size, gamma=1.0, threshold=0.01):\n",
    "    states = np.arange(grid_size**2)  # Array of all states from 0 to grid_size^2 - 1\n",
    "    V = np.zeros(len(states))  # Initialize value function array to zero\n",
    "    delta = float('inf')  # Large initial value for tracking convergence\n",
    "    actions = ['up', 'down', 'left', 'right']  # Possible actions\n",
    "    terminal_states = {0, grid_size**2 - 1}  # Terminal states (upper-left and lower-right)\n",
    "\n",
    "    while delta > threshold:  # Continue until change in value function is small\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if s in terminal_states:  # Skip terminal states\n",
    "                continue\n",
    "            v = V[s]  # Current value of state s\n",
    "            # Compute the maximum value of all actions from state s\n",
    "            V[s] = max([sum([p * (r + gamma * V[next_state(s, a, grid_size)]) for p, r in [(1.0, -1)]])\n",
    "                        for a in actions])\n",
    "            # Update delta to be the maximum difference observed in this iteration\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "    return V  # Return the final value function\n",
    "\n",
    "# Example usage:\n",
    "grid_size = 4  # 4x4 grid\n",
    "optimal_values = value_iteration(grid_size, 0.99, 0.01)  # Run value iteration\n",
    "\n",
    "print(\"Optimal values for each state in a {}x{} grid:\".format(grid_size, grid_size))\n",
    "for i in range(grid_size):\n",
    "    print([f\"{v:.2f}\" for v in optimal_values[i*grid_size:(i+1)*grid_size]])  # Print formatted values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0b4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
