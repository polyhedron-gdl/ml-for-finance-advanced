{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e809ff",
   "metadata": {},
   "source": [
    "Policy Evaluation is a fundamental concept in reinforcement learning, particularly within the framework of dynamic programming. It refers to the process of computing the value of a given policy. Let's break it down step-by-step for clarity:\n",
    "\n",
    "1. **Policy**: In the context of reinforcement learning, a policy is a strategy that an agent follows to decide actions based on its current state. Formally, a policy is a mapping from states of the world to actions to be taken when in those states.\n",
    "\n",
    "2. **Value of a Policy**: The value of a policy, denoted as $ V^\\pi(s) $, measures how good it is to follow the policy $ \\pi $ from a particular state $ s $. This value is calculated as the expected return starting from state $ s $ and following $ \\pi $ thereafter. The return is the cumulative sum of rewards the agent receives, which may be discounted over time by a factor $ \\gamma $ (where $ 0 \\leq \\gamma \\leq 1 $).\n",
    "\n",
    "3. **The Goal of Policy Evaluation**: The objective is to determine the value function $ V^\\pi $ for a given policy $ \\pi $. This function provides the expected return for each state under $ \\pi $.\n",
    "\n",
    "4. **Bellman Equation for Policy Evaluation**: To find $ V^\\pi(s) $, we use the Bellman equation for policy evaluation. It expresses the value of a state $ s $ under policy $ \\pi $ as the sum of the immediate reward plus the discounted value of the next state. The equation is:\n",
    "   $$\n",
    "   V^\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma V^\\pi(s') \\right]\n",
    "   $$\n",
    "   where $ \\pi(a|s) $ is the probability of taking action $ a $ in state $ s $ under policy $ \\pi $, and $ p(s', r | s, a) $ is the probability of transitioning to state $ s' $ and receiving reward $ r $ after taking action $ a $ in state $ s $.\n",
    "\n",
    "5. **Iterative Approach**: Since $ V^\\pi $ appears on both sides of the Bellman equation, the equation is typically solved iteratively. Starting with an initial guess for $ V^\\pi $ (often zeros), the values are updated repeatedly using the above equation until the changes in the value function between iterations are sufficiently small (a process known as convergence).\n",
    "\n",
    "6. **Convergence**: Under certain conditions (e.g., all states are visited infinitely often), this iterative process converges to the true value function for policy $ \\pi $, denoted $ V^\\pi $.\n",
    "\n",
    "In summary, policy evaluation in dynamic programming for reinforcement learning involves calculating the value function $ V^\\pi $ that quantifies the expected returns for following a policy $ \\pi $ from each state. This process is crucial for understanding the effectiveness of a policy and serves as a building block for more complex algorithms like policy iteration and value iteration, where the policy is improved iteratively based on these evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36018dff",
   "metadata": {},
   "source": [
    "Certainly! Let’s explore a simple example of policy evaluation using the Bellman equation to make the concept more concrete. We'll consider a simplified gridworld scenario where an agent can move in four directions: north, south, east, and west.\n",
    "\n",
    "### Scenario Setup:\n",
    "\n",
    "- **Grid**: A 2x2 grid with states labeled as S1, S2, S3, and S4.\n",
    "- **Actions**: The agent can choose from four actions in each state: move north, south, east, or west. If a move would take the agent off the grid, it remains in the current state.\n",
    "- **Rewards**: Moving to S4 gives a reward of +1. Any other move gives a reward of 0.\n",
    "- **Policy $ \\pi $**: A deterministic policy where:\n",
    "  - From S1, move east to S2.\n",
    "  - From S2, move east to S3.\n",
    "  - From S3, move east to S4 (though it's actually south in a 2x2 grid, for simplicity we'll assume east).\n",
    "  - From S4, stay in S4.\n",
    "\n",
    "### Objective:\n",
    "Calculate the value function $ V^\\pi(s) $ for each state under the given policy.\n",
    "\n",
    "### Bellman Equation for Policy Evaluation:\n",
    "The Bellman equation in this context will be:\n",
    "$$\n",
    "V^\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma V^\\pi(s') \\right]\n",
    "$$\n",
    "- $ \\pi(a|s) $ is 1 for the deterministic choice specified by the policy and 0 otherwise.\n",
    "- $ p(s', r | s, a) $ is the transition probability and reward received.\n",
    "\n",
    "### Calculation:\n",
    "Assuming a discount factor $ \\gamma = 0.9 $:\n",
    "\n",
    "1. **Initial Values**: Let’s start with $ V^\\pi(s) = 0 $ for all states.\n",
    "\n",
    "2. **Iterative Update**:\n",
    "   - For **S1**:\n",
    "     $$\n",
    "     V^\\pi(S1) = 1 \\times \\left[ 0 + 0.9 \\times V^\\pi(S2) \\right] = 0.9 \\times V^\\pi(S2)\n",
    "     $$\n",
    "   - For **S2**:\n",
    "     $$\n",
    "     V^\\pi(S2) = 1 \\times \\left[ 0 + 0.9 \\times V^\\pi(S3) \\right] = 0.9 \\times V^\\pi(S3)\n",
    "     $$\n",
    "   - For **S3**:\n",
    "     $$\n",
    "     V^\\pi(S3) = 1 \\times \\left[ 0 + 0.9 \\times V^\\pi(S4) \\right] = 0.9 \\times V^\\pi(S4)\n",
    "     $$\n",
    "   - For **S4** (goal state):\n",
    "     $$\n",
    "     V^\\pi(S4) = 1 \\times \\left[ 1 + 0.9 \\times V^\\pi(S4) \\right]\n",
    "     $$\n",
    "     Solving this equation $ V^\\pi(S4) = 1 + 0.9 \\times V^\\pi(S4) $, we get $ V^\\pi(S4) = 10 $.\n",
    "\n",
    "3. **Back-substitution**:\n",
    "   - With $ V^\\pi(S4) = 10 $, update $ S3 $, $ S2 $, and $ S1 $:\n",
    "     $$\n",
    "     V^\\pi(S3) = 0.9 \\times 10 = 9\n",
    "     $$\n",
    "     $$\n",
    "     V^\\pi(S2) = 0.9 \\times 9 = 8.1\n",
    "     $$\n",
    "     $$\n",
    "     V^\\pi(S1) = 0.9 \\times 8.1 = 7.29\n",
    "     $$\n",
    "\n",
    "### Conclusion:\n",
    "Now, we have calculated the value function for each state under the policy $ \\pi $ using the Bellman equation. The results reflect the expected returns starting from each state and following the policy to reach the terminal state S4. This calculation helps understand the effectiveness of the policy and the utility of states within the context of the chosen actions and rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b1fbc",
   "metadata": {},
   "source": [
    "Finding the optimal policy in reinforcement learning, particularly through the lens of dynamic programming, is one of the key objectives in many decision-making scenarios. The optimal policy is the one that yields the highest expected return from any given state. Here are some of the main methods to find the optimal policy:\n",
    "\n",
    "### 1. **Value Iteration**\n",
    "\n",
    "Value iteration is a powerful and widely used method to determine the optimal policy. It involves two main steps that are repeated iteratively:\n",
    "\n",
    "- **Value Update Step**: Update the value function by using the Bellman optimality equation:\n",
    "  \\[\n",
    "  V(s) \\leftarrow \\max_a \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma V(s') \\right]\n",
    "  \\]\n",
    "  This step involves calculating the maximum expected return from each state, considering all possible actions and updating the value function based on these maximal returns.\n",
    "\n",
    "- **Policy Derivation**: Once the value function has converged to \\( V^*(s) \\), the optimal value function, the optimal policy \\( \\pi^*(a|s) \\) can be derived directly by choosing the action that maximizes the expected return for each state:\n",
    "  \\[\n",
    "  \\pi^*(s) = \\arg \\max_a \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma V^*(s') \\right]\n",
    "  \\]\n",
    "\n",
    "### 2. **Policy Iteration**\n",
    "\n",
    "Policy iteration is another common approach that alternates between evaluating a given policy and improving it:\n",
    "\n",
    "- **Policy Evaluation**: Calculate the value function \\( V^\\pi(s) \\) for an initial policy \\( \\pi \\) using the method of policy evaluation as discussed previously.\n",
    "\n",
    "- **Policy Improvement**: Generate a new policy \\( \\pi'\\) by making it greedy with respect to the value function derived in the evaluation phase. This means for each state, select the action that maximizes the expected return:\n",
    "  \\[\n",
    "  \\pi'(s) = \\arg \\max_a \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma V^\\pi(s') \\right]\n",
    "  \\]\n",
    "\n",
    "- **Iteration**: Replace \\( \\pi \\) with \\( \\pi' \\) and repeat the evaluation and improvement steps until the policy no longer changes, indicating convergence to an optimal policy.\n",
    "\n",
    "### 3. **Q-learning (Model-free method)**\n",
    "\n",
    "Q-learning is a model-free approach, meaning it does not require knowledge of the transition probabilities and rewards. It directly estimates the optimal action-value function \\( Q^*(s, a) \\):\n",
    "\n",
    "- **Q-value Update**: Update the Q-values based on the formula:\n",
    "  \\[\n",
    "  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "  \\]\n",
    "  where \\( \\alpha \\) is the learning rate.\n",
    "\n",
    "- **Policy Derivation**: Once the Q-values have sufficiently converged, derive the optimal policy by selecting the action with the highest Q-value in each state:\n",
    "  \\[\n",
    "  \\pi^*(s) = \\arg \\max_a Q^*(s, a)\n",
    "  \\]\n",
    "\n",
    "Each of these methods—value iteration, policy iteration, and Q-learning—provides a systematic way to find the optimal policy by utilizing the principles of dynamic programming or approximation in the case of Q-learning. The choice of method often depends on the specifics of the problem, including whether a model of the environment is available (model-based vs. model-free) and the size and complexity of the state and action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049200d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
